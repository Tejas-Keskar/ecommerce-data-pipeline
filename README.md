# ğŸš€ E-Commerce Data Pipeline with Apache Airflow & Spark  

## ğŸ“Œ Project Overview  

This project automates the **ETL (Extract, Transform, Load) pipeline** for e-commerce event data using **Apache Airflow**, **Apache Spark**, **HDFS**, and **MySQL**. The pipeline detects new data files, processes them using Spark, and loads the cleaned data into a MySQL database for analytics. Additionally, a **Power BI dashboard** is created for visualization and insights.  

---

## ğŸ—ï¸ Architecture  

The pipeline follows these steps:  
1ï¸âƒ£ **ğŸ“‚ File Detection** â†’ Airflow detects new files in the source folder.  
2ï¸âƒ£ **ğŸ“¤ Move to HDFS** â†’ Files are moved to Hadoop Distributed File System (HDFS).  
3ï¸âƒ£ **âš¡ Spark Processing** â†’ Spark processes the data (cleaning, transformations, aggregations).  
4ï¸âƒ£ **ğŸ—„ï¸ Load to MySQL** â†’ The processed data is stored in MySQL for analysis.  
5ï¸âƒ£ **ğŸ“Š Power BI Dashboard** â†’ Data is visualized in an interactive dashboard.  
6ï¸âƒ£ **ğŸ—‘ï¸ Cleanup** â†’ The processed files are deleted from the local system.  

---

## ğŸ“ Project Structure  

```bash
ecommerce-data-pipeline/
â”‚â”€â”€ dags/
â”‚   â”œâ”€â”€ ecommerce_dag.py         
â”‚â”€â”€ scripts/
â”‚   â”œâ”€â”€ data_processing.py       
â”‚â”€â”€ dashboards/
â”‚   â”œâ”€â”€ powerbi_dashboard.pbix   
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ sample_data.csv          
â”‚â”€â”€ README.md      
â”‚â”€â”€ requirements.txt              
```

---

## ğŸ›  Technologies Used  

ğŸ”¹ **Apache Airflow** â€“ For scheduling and managing the pipeline.  
ğŸ”¹ **Apache Spark** â€“ For data transformation and processing.  
ğŸ”¹ **HDFS** â€“ For distributed file storage.  
ğŸ”¹ **MySQL** â€“ For storing processed data.  
ğŸ”¹ **Python** â€“ The primary programming language.  
ğŸ”¹ **Power BI** â€“ For data visualization and insights.  

---

## âš™ï¸ Setup & Installation

1ï¸âƒ£ **Install dependencies:**  
Ensure you have Python 3.8+, Apache Airflow, Spark, Hadoop, and MySQL installed.
```bash
pip install -r requirements.txt
```
2ï¸âƒ£ **Start Airflow**
```bash
airflow db init
airflow webserver --port 8080
airflow scheduler
```
3ï¸âƒ£ **Submit Spark Job Manually (Optional)**
```bash
spark-submit --master local[*] scripts/data_processing.py
```
4ï¸âƒ£ **Run DAG in Airflow UI**
1. Open http://localhost:8080
2. Enable & Trigger the ecommerce_data_pipeline DAG

---

## ğŸ“Š Database Schema
### 1ï¸âƒ£ eCommerce Events Table (`ecommerce_events`)

Stores raw event data from the e-commerce platform.

| Column Name     | Data Type  | Description                          |
|----------------|-----------|--------------------------------------|
| event_time     | TIMESTAMP | Event timestamp                     |
| event_type     | VARCHAR   | Type of event (view, cart, purchase)|
| product_id     | INT       | Unique product identifier           |
| category_id    | VARCHAR   | Category identifier                 |
| category_code  | VARCHAR   | Category name                       |
| brand         | VARCHAR   | Product brand                        |
| price         | FLOAT     | Product price                        |
| user_id       | INT       | User identifier                      |
| user_session  | VARCHAR   | User session ID                      |

### 2ï¸âƒ£ User Engagement Table (`user_engagement`)

Aggregates user interactions.

| Column Name          | Data Type  | Description                               |
|----------------------|-----------|-------------------------------------------|
| user_id             | INT       | Unique user ID                            |
| total_views        | INT       | Number of product views                   |
| total_add_to_cart  | INT       | Number of products added to cart          |
| total_purchases    | INT       | Number of purchases                       |
| view_to_cart_rate  | FLOAT     | Conversion rate from view to cart         |
| cart_to_purchase_rate | FLOAT  | Conversion rate from cart to purchase     |

### 3ï¸âƒ£ Session Revenue Table (`session_revenue`)

Stores revenue per session.

| Column Name    | Data Type  | Description                          |
|---------------|-----------|--------------------------------------|
| user_session  | VARCHAR   | Session ID                          |
| total_revenue | FLOAT     | Total revenue generated in session  |

---

## ğŸ’¡ Future Improvements

ğŸ”¹ Implement Star Schema for better analytics and query performance
ğŸ”¹ Enable real-time streaming with Kafka for live event tracking

---

## ğŸ“ Contact  
If you have any questions or suggestions, feel free to reach out:  
ğŸ“§ **Email:** keskartejas01@gmail.com  
ğŸ“Œ **LinkedIn:** https://www.linkedin.com/in/tejas-keskar-329634288